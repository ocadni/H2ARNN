
@article{10.48550/arxiv.1512.03385,
  year      = {2015},
  title     = {{Deep Residual Learning for Image Recognition}},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal   = {arXiv},
  doi       = {10.48550/arxiv.1512.03385},
  eprint    = {1512.03385},
  abstract  = {{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2015/arXiv-2015-He.pdf}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}


@article{10.1038/s42256-021-00401-3,
  year      = {2021},
  keywords  = {autoregressive},
  title     = {{Variational neural annealing}},
  author    = {Hibat-Allah, Mohamed and Inack, Estelle M. and Wiersema, Roeland and Melko, Roger G. and Carrasquilla, Juan},
  journal   = {Nature Machine Intelligence},
  doi       = {10.1038/s42256-021-00401-3},
  eprint    = {2101.10154},
  url       = {https://www.nature.com/articles/s42256-021-00401-3},
  abstract  = {{Many important challenges in science and technology can be cast as optimization problems. When viewed in a statistical physics framework, these can be tackled by simulated annealing, where a gradual cooling procedure helps search for ground-state solutions of a target Hamiltonian. Although powerful, simulated annealing is known to have prohibitively slow sampling dynamics when the optimization landscape is rough or glassy. Here we show that, by generalizing the target distribution with a parameterized model, an analogous annealing framework based on the variational principle can be used to search for ground-state solutions. Modern autoregressive models such as recurrent neural networks provide ideal parameterizations because they can be sampled exactly without slow dynamics, even when the model encodes a rough landscape. We implement this procedure in the classical and quantum settings on several prototypical spin glass Hamiltonians and find that, on average, it substantially outperforms traditional simulated annealing in the asymptotic limit, illustrating the potential power of this yet unexplored route to optimization. Optimization problems can be described in terms of a statistical physics framework. This offers the possibility to make use of ‘simulated annealing’, which is a procedure to search for a target solution similar to the gradual cooling of a condensed matter system to its ground state. The approach can now be sped up significantly by implementing a model of recurrent neural networks, in a new strategy called variational neural annealing.}},
  pages     = {1--10},
  number    = {11},
  volume    = {3},
  month     = {10},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2021/Nature%20Machine%20Intelligence-2021-Hibat-Allah.pdf}
}

@misc{pippo2021,
  year     = {2021},
  title    = {For groundbreaking contributions to our understanding of complex physical systems},
  author   = {The nobel committee for Physics},
  journal  = {Fri},
  keywords = {},
  url      = {https://www.nobelprize.org/prizes/physics/2021/advanced-information/}
}


@article{10.1142/0271,
  year      = {1986},
  title     = {{Spin Glass Theory and Beyond}},
  author    = {Mezard, M and Parisi, G and Virasoro, M},
  journal   = {World Scientific Lecture Notes in Physics},
  issn      = {1793-1436},
  doi       = {10.1142/0271},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/1986/World%20Scientific%20Lecture%20Notes%20in%20Physics-1986-Mezard_1.pdf}
}


@article{bengioNatureDeepLearning2015,
  abstract      = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  author        = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  da            = {2015/05/01},
  date-added    = {2023-01-30 10:56:48 +0100},
  date-modified = {2023-01-30 10:56:48 +0100},
  doi           = {10.1038/nature14539},
  id            = {LeCun2015},
  isbn          = {1476-4687},
  journal       = {Nature},
  number        = {7553},
  pages         = {436--444},
  title         = {Deep learning},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/nature14539},
  volume        = {521},
  year          = {2015},
  bdsk-url-1    = {https://doi.org/10.1038/nature14539}
}

@article{noe2019boltzmann,
  title     = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
  author    = {No{\'e}, Frank and Olsson, Simon and K{\"o}hler, Jonas and Wu, Hao},
  journal   = {Science},
  volume    = {365},
  number    = {6457},
  pages     = {eaaw1147},
  year      = {2019},
  publisher = {American Association for the Advancement of Science}
}

@article{jumper2021highly,
  title     = {Highly accurate protein structure prediction with AlphaFold},
  author    = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal   = {Nature},
  volume    = {596},
  number    = {7873},
  pages     = {583--589},
  year      = {2021},
  publisher = {Nature Publishing Group UK London}
}


@article{Carleo2017,
  year      = {2017},
  title     = {{Solving the quantum many-body problem with artificial neural networks}},
  author    = {Carleo, Giuseppe and Troyer, Matthias},
  doi       = {10.1126/science.aag2302},
  pmid      = {28183973},
  eprint    = {1606.02318},
  url       = {http://science.sciencemag.org/},
  abstract  = {{The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.}},
  pages     = {602--606},
  volume    = {355},
  note      = {www.sciencemag.org/content/355/6325/602/suppl/},
  keywords  = {},
  month     = {2},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2017/10e35128-6b60-4cdf-1237-a77bea0237ac_1.pdf}
}

@article{RevModPhys.91.045002,
  title = {Machine learning and the physical sciences},
  author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborov\'a, Lenka},
  journal = {Rev. Mod. Phys.},
  volume = {91},
  issue = {4},
  pages = {045002},
  numpages = {39},
  year = {2019},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.91.045002},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.91.045002}
}