
@article{10.48550/arxiv.1512.03385,
  year      = {2015},
  title     = {{Deep Residual Learning for Image Recognition}},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal   = {arXiv},
  doi       = {10.48550/arxiv.1512.03385},
  eprint    = {1512.03385},
  abstract  = {{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2015/arXiv-2015-He.pdf}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{10.1038/s42256-021-00401-3,
  year      = {2021},
  keywords  = {autoregressive},
  title     = {{Variational neural annealing}},
  author    = {Hibat-Allah, Mohamed and Inack, Estelle M. and Wiersema, Roeland and Melko, Roger G. and Carrasquilla, Juan},
  journal   = {Nature Machine Intelligence},
  doi       = {10.1038/s42256-021-00401-3},
  eprint    = {2101.10154},
  url       = {https://www.nature.com/articles/s42256-021-00401-3},
  abstract  = {{Many important challenges in science and technology can be cast as optimization problems. When viewed in a statistical physics framework, these can be tackled by simulated annealing, where a gradual cooling procedure helps search for ground-state solutions of a target Hamiltonian. Although powerful, simulated annealing is known to have prohibitively slow sampling dynamics when the optimization landscape is rough or glassy. Here we show that, by generalizing the target distribution with a parameterized model, an analogous annealing framework based on the variational principle can be used to search for ground-state solutions. Modern autoregressive models such as recurrent neural networks provide ideal parameterizations because they can be sampled exactly without slow dynamics, even when the model encodes a rough landscape. We implement this procedure in the classical and quantum settings on several prototypical spin glass Hamiltonians and find that, on average, it substantially outperforms traditional simulated annealing in the asymptotic limit, illustrating the potential power of this yet unexplored route to optimization. Optimization problems can be described in terms of a statistical physics framework. This offers the possibility to make use of ‘simulated annealing’, which is a procedure to search for a target solution similar to the gradual cooling of a condensed matter system to its ground state. The approach can now be sped up significantly by implementing a model of recurrent neural networks, in a new strategy called variational neural annealing.}},
  pages     = {1--10},
  number    = {11},
  volume    = {3},
  month     = {10},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2021/Nature%20Machine%20Intelligence-2021-Hibat-Allah.pdf}
}

@online{Nobel2021a,
  author  = {The nobel committee for Physics},
  title   = {For groundbreaking contributions to our understanding of complex physical systems.},
  year    = 1999,
  url     = {http://web.archive.org/web/20080207010024/http://www.808multimedia.com/winnt/kernel.htm},
  urldate = {2010-09-30}
}

@article{PhysRevLett.35.1792,
  title     = {Solvable Model of a Spin-Glass},
  author    = {Sherrington, David and Kirkpatrick, Scott},
  journal   = {Phys. Rev. Lett.},
  volume    = {35},
  issue     = {26},
  pages     = {1792--1796},
  numpages  = {0},
  year      = {1975},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.35.1792},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792}
}

@article{PhysRevLett.43.1754,
  title     = {Infinite Number of Order Parameters for Spin-Glasses},
  author    = {Parisi, G.},
  journal   = {Phys. Rev. Lett.},
  volume    = {43},
  issue     = {23},
  pages     = {1754--1756},
  numpages  = {0},
  year      = {1979},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.43.1754},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.43.1754}
}

@article{doi:10.1126/science.1073287,
  author   = {M. Mézard  and G. Parisi  and R. Zecchina },
  title    = {Analytic and Algorithmic Solution of Random Satisfiability Problems},
  journal  = {Science},
  number   = {5582},
  pages    = {812-815},
  year     = {2002},
  doi      = {10.1126/science.1073287},
  url      = {https://www.science.org/doi/abs/10.1126/science.1073287},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.1073287},
  abstract = {We study the satisfiability of random Boolean expressions built from many clauses with K variables per clause (K-satisfiability). Expressions with a ratio α of clauses to variables less than a threshold αc are almost always satisfiable, whereas those with a ratio above this threshold are almost always unsatisfiable. We show the existence of an intermediate phase below αc, where the proliferation of metastable states is responsible for the onset of complexity in search algorithms. We introduce a class of optimization algorithms that can deal with these metastable states; one such algorithm has been tested successfully on the largest existing benchmark of K-satisfiability.}
}


@article{Parisi_1980,
  doi       = {10.1088/0305-4470/13/4/009},
  url       = {https://dx.doi.org/10.1088/0305-4470/13/4/009},
  year      = {1980},
  month     = {apr},
  publisher = {},
  volume    = {13},
  number    = {4},
  pages     = {L115},
  author    = {G Parisi},
  title     = {A sequence of approximated solutions to the S-K model for spin glasses},
  journal   = {Journal of Physics A: Mathematical and General},
  abstract  = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.}
}


@article{PARISI1979203,
  title    = {Toward a mean field theory for spin glasses},
  journal  = {Physics Letters A},
  volume   = {73},
  number   = {3},
  pages    = {203-205},
  year     = {1979},
  issn     = {0375-9601},
  doi      = {https://doi.org/10.1016/0375-9601(79)90708-4},
  url      = {https://www.sciencedirect.com/science/article/pii/0375960179907084},
  author   = {G. Parisi},
  abstract = {We find an approximate solution of the Sherrington-Kirkpatrick model for spin glasses; the internal energy and the specific heat are in very good agreement with the computer simulations, the zero temperature entropy is unfortunately negative, although it is very small.}
}

@misc{Nobel2021,
  year     = {2021},
  title    = {For groundbreaking contributions to our understanding of complex physical systems. [Nobel to G. Parisi]},
  author   = {The nobel committee for Physics},
  journal  = {Fri},
  keywords = {},
  url      = {https://www.nobelprize.org/prizes/physics/2021/advanced-information/}
}

@book{10.1142/0271,
  year      = {1986},
  title     = {{Spin Glass Theory and Beyond}},
  author    = {Mezard, M and Parisi, G and Virasoro, M},
  journal   = {World Scientific Lecture Notes in Physics},
  issn      = {1793-1436},
  doi       = {10.1142/0271},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/1986/World%20Scientific%20Lecture%20Notes%20in%20Physics-1986-Mezard_1.pdf}
}


@article{bengioNatureDeepLearning2015,
  abstract      = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  author        = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  da            = {2015/05/01},
  date-added    = {2023-01-30 10:56:48 +0100},
  date-modified = {2023-01-30 10:56:48 +0100},
  doi           = {10.1038/nature14539},
  id            = {LeCun2015},
  isbn          = {1476-4687},
  journal       = {Nature},
  number        = {7553},
  pages         = {436--444},
  title         = {Deep learning},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/nature14539},
  volume        = {521},
  year          = {2015},
  bdsk-url-1    = {https://doi.org/10.1038/nature14539}
}

@article{noe2019boltzmann,
  title     = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
  author    = {No{\'e}, Frank and Olsson, Simon and K{\"o}hler, Jonas and Wu, Hao},
  journal   = {Science},
  volume    = {365},
  number    = {6457},
  pages     = {eaaw1147},
  year      = {2019},
  publisher = {American Association for the Advancement of Science}
}

@article{jumper2021highly,
  title     = {Highly accurate protein structure prediction with AlphaFold},
  author    = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal   = {Nature},
  volume    = {596},
  number    = {7873},
  pages     = {583--589},
  year      = {2021},
  publisher = {Nature Publishing Group UK London}
}

@article{doi:10.1126/science.aag2302,
  author   = {Giuseppe Carleo  and Matthias Troyer },
  title    = {Solving the quantum many-body problem with artificial neural networks},
  journal  = {Science},
  volume   = {355},
  number   = {6325},
  pages    = {602-606},
  year     = {2017},
  doi      = {10.1126/science.aag2302},
  url      = {https://www.science.org/doi/abs/10.1126/science.aag2302},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.aag2302},
  abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.}
}


@article{RevModPhys.91.045002,
  title     = {Machine learning and the physical sciences},
  author    = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborov\'a, Lenka},
  journal   = {Rev. Mod. Phys.},
  volume    = {91},
  issue     = {4},
  pages     = {045002},
  numpages  = {39},
  year      = {2019},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/RevModPhys.91.045002},
  url       = {https://link.aps.org/doi/10.1103/RevModPhys.91.045002}
} 


@article{Carrasquilla2017,
  abstract      = {The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order.},
  author        = {Carrasquilla, Juan and Melko, Roger G.},
  da            = {2017/05/01},
  date-added    = {2023-01-31 10:03:06 +0100},
  date-modified = {2023-01-31 10:03:06 +0100},
  doi           = {10.1038/nphys4035},
  id            = {Carrasquilla2017},
  isbn          = {1745-2481},
  journal       = {Nature Physics},
  number        = {5},
  pages         = {431--434},
  title         = {Machine learning phases of matter},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/nphys4035},
  volume        = {13},
  year          = {2017},
  bdsk-url-1    = {https://doi.org/10.1038/nphys4035}
}


@article{Nieuwenburg2017,
  abstract      = {A neural-network technique can exploit the power of machine learning to mine the exponentially large data sets characterizing the state space of condensed-matter systems. Topological transitions and many-body localization are first on the list.},
  author        = {van Nieuwenburg, Evert P. L. and Liu, Ye-Hua and Huber, Sebastian D.},
  da            = {2017/05/01},
  date-added    = {2023-01-31 10:06:55 +0100},
  date-modified = {2023-01-31 10:06:55 +0100},
  doi           = {10.1038/nphys4037},
  id            = {van Nieuwenburg2017},
  isbn          = {1745-2481},
  journal       = {Nature Physics},
  number        = {5},
  pages         = {435--439},
  title         = {Learning phase transitions by confusion},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/nphys4037},
  volume        = {13},
  year          = {2017},
  bdsk-url-1    = {https://doi.org/10.1038/nphys4037}
}


@article{Wu2019,
  year      = {2019},
  title     = {{Solving Statistical Mechanics Using Variational Autoregressive Networks}},
  author    = {Wu, Dian and Wang, Lei and Zhang, Pan},
  journal   = {Physical Review Letters},
  issn      = {0031-9007},
  doi       = {10.1103/physrevlett.122.080602},
  pmid      = {30932595},
  eprint    = {1809.10606},
  url       = {http://arxiv.org/abs/1809.10606},
  abstract  = {{We propose a general framework for solving stati stical mechanics of systems with finite size. The approach extends the celebrated variational mean-field approaches using autoregressive neural networks, which support direct sampling and exact calculation of normalized probability of configurations. It computes variational free energy, estimates physical quantities such as entropy, magnetizations and correlations, and generates uncorrelated samples all at once. Training of the network employs the policy gradient approach in reinforcement learning, which unbiasedly estimates the gradient of variational parameters. We apply our approach to several classic systems, including 2D Ising models, the Hopfield model, the Sherrington-Kirkpatrick model, and the inverse Ising model, for demonstrating its advantages over existing variational mean-field methods. Our approach sheds light on solving statistical physics problems using modern deep generative neural networks.}},
  pages     = {1--8},
  number    = {8},
  volume    = {122},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2019/Physical%20Review%20Letters-2019-Wu_1.pdf}
}


@article{Biazzo2022,
  abstract      = {The reconstruction of missing information in epidemic spreading on contact networks can be essential in the prevention and containment strategies. The identification and warning of infectious but asymptomatic individuals (i.e., contact tracing), the well-known patient-zero problem, or the inference of the infectivity values in structured populations are examples of significant epidemic inference problems. As the number of possible epidemic cascades grows exponentially with the number of individuals involved and only an almost negligible subset of them is compatible with the observations (e.g., medical tests), epidemic inference in contact networks poses incredible computational challenges. We present a new generative neural networks framework that learns to generate the most probable infection cascades compatible with observations. The proposed method achieves better (in some cases, significantly better) or comparable results with existing methods in all problems considered both in synthetic and real contact networks. Given its generality, clear Bayesian and variational nature, the presented framework paves the way to solve fundamental inference epidemic problems with high precision in small and medium-sized real case scenarios such as the spread of infections in workplaces and hospitals.},
  author        = {Biazzo, Indaco and Braunstein, Alfredo and Dall'Asta, Luca and Mazza, Fabio},
  da            = {2022/11/16},
  date-added    = {2023-01-31 10:24:18 +0100},
  date-modified = {2023-01-31 10:24:18 +0100},
  doi           = {10.1038/s41598-022-20898-x},
  id            = {Biazzo2022},
  isbn          = {2045-2322},
  journal       = {Scientific Reports},
  number        = {1},
  pages         = {19673},
  title         = {A Bayesian generative neural network framework for epidemic inference problems},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/s41598-022-20898-x},
  volume        = {12},
  year          = {2022},
  bdsk-url-1    = {https://doi.org/10.1038/s41598-022-20898-x}
}

@article{doi:10.1073/pnas.79.8.2554,
  author   = {J J Hopfield },
  title    = {Neural networks and physical systems with emergent collective computational abilities.},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {79},
  number   = {8},
  pages    = {2554-2558},
  year     = {1982},
  doi      = {10.1073/pnas.79.8.2554},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}
}

@article{PhysRevA.32.1007,
  title     = {Spin-glass models of neural networks},
  author    = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
  journal   = {Phys. Rev. A},
  volume    = {32},
  issue     = {2},
  pages     = {1007--1018},
  numpages  = {0},
  year      = {1985},
  month     = {Aug},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevA.32.1007},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.32.1007}
}

@article{PhysRevLett.102.195701,
  title     = {Theory of Amorphous Packings of Binary Mixtures of Hard Spheres},
  author    = {Biazzo, Indaco and Caltagirone, Francesco and Parisi, Giorgio and Zamponi, Francesco},
  journal   = {Phys. Rev. Lett.},
  volume    = {102},
  issue     = {19},
  pages     = {195701},
  numpages  = {4},
  year      = {2009},
  month     = {May},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.102.195701},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.102.195701}
}

@article{Gardner_1987,
  doi       = {10.1209/0295-5075/4/4/016},
  url       = {https://dx.doi.org/10.1209/0295-5075/4/4/016},
  year      = {1987},
  month     = {aug},
  publisher = {},
  volume    = {4},
  number    = {4},
  pages     = {481},
  author    = {E. Gardner},
  title     = {Maximum Storage Capacity in Neural Networks},
  journal   = {Europhysics Letters},
  abstract  = {The upper storage capacity of a neural network for patterns of fixed magnetization m is calculated. The optimal capacity increases with the correlation m2 between the patterns.}
}


@article{doi:10.1080/00018732.2016.1211393,
  author    = {Lenka Zdeborová and Florent Krzakala},
  title     = {Statistical physics of inference: thresholds and algorithms},
  journal   = {Advances in Physics},
  volume    = {65},
  number    = {5},
  pages     = {453-552},
  year      = {2016},
  publisher = {Taylor & Francis},
  doi       = {10.1080/00018732.2016.1211393},
  url       = {https://doi.org/10.1080/00018732.2016.1211393},
  eprint    = {https://doi.org/10.1080/00018732.2016.1211393}
}

@article{RevModPhys.82.789,
  title     = {Mean-field theory of hard sphere glasses and jamming},
  author    = {Parisi, Giorgio and Zamponi, Francesco},
  journal   = {Rev. Mod. Phys.},
  volume    = {82},
  issue     = {1},
  pages     = {789--845},
  numpages  = {0},
  year      = {2010},
  month     = {Mar},
  publisher = {American Physical Society},
  doi       = {10.1103/RevModPhys.82.789},
  url       = {https://link.aps.org/doi/10.1103/RevModPhys.82.789}
}

@article{Nguyen2017,
  annote        = {doi: 10.1080/00018732.2017.1341604},
  author        = {Nguyen, H. Chau and Zecchina, Riccardo and Berg, Johannes},
  booktitle     = {Advances in Physics},
  da            = {2017/07/03},
  date          = {2017/07/03},
  date-added    = {2023-01-31 11:50:02 +0100},
  date-modified = {2023-01-31 11:50:02 +0100},
  doi           = {10.1080/00018732.2017.1341604},
  isbn          = {0001-8732},
  journal       = {Advances in Physics},
  journal1      = {Advances in Physics},
  m3            = {doi: 10.1080/00018732.2017.1341604},
  month         = {07},
  number        = {3},
  pages         = {197--261},
  publisher     = {Taylor \& Francis},
  title         = {Inverse statistical problems: from the inverse Ising problem to data science},
  ty            = {JOUR},
  url           = {https://doi.org/10.1080/00018732.2017.1341604},
  volume        = {66},
  year          = {2017},
  year1         = {2017},
  bdsk-url-1    = {https://doi.org/10.1080/00018732.2017.1341604}
}

@article{Chaudhari_2019,
  doi       = {10.1088/1742-5468/ab39d9},
  url       = {https://dx.doi.org/10.1088/1742-5468/ab39d9},
  year      = {2019},
  month     = {dec},
  publisher = {IOP Publishing and SISSA},
  volume    = {2019},
  number    = {12},
  pages     = {124018},
  author    = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  title     = {Entropy-SGD: biasing gradient descent into wide valleys*},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  abstract  = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.}
}

@misc{https://doi.org/10.48550/arxiv.1506.00019,
  doi       = {10.48550/ARXIV.1506.00019},
  url       = {https://arxiv.org/abs/1506.00019},
  author    = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  keywords  = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{PhysRevLett.55.1530,
  title     = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
  author    = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
  journal   = {Phys. Rev. Lett.},
  volume    = {55},
  issue     = {14},
  pages     = {1530--1533},
  numpages  = {0},
  year      = {1985},
  month     = {Sep},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.55.1530},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.55.1530}
}



@inproceedings{pmlr-v37-sohl-dickstein15,
  title     = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author    = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages     = {2256--2265},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url       = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract  = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}


@inproceedings{pmlr-v37-germain15,
  title     = {MADE: Masked Autoencoder for Distribution Estimation},
  author    = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages     = {881--889},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/germain15.pdf},
  url       = {https://proceedings.mlr.press/v37/germain15.html},
  abstract  = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}

@inproceedings{NIPS2017_3f5ee243,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}


@article{10.1103/physreve.101.023304,
  year      = {2020},
  title     = {{Asymptotically unbiased estimation of physical observables with neural samplers}},
  author    = {Nicoli, Kim A and Nakajima, Shinichi and Strodthoff, Nils and Samek, Wojciech and Müller, Klaus-Robert and Kessel, Pan},
  journal   = {Physical Review E},
  issn      = {2470-0045},
  doi       = {10.1103/physreve.101.023304},
  pmid      = {32168605},
  eprint    = {1910.13496},
  abstract  = {{We propose a general framework for the estimation of observables with generative neural samplers focusing on modern deep generative neural networks that provide an exact sampling probability. In this framework, we present asymptotically unbiased estimators for generic observables, including those that explicitly depend on the partition function such as free energy or entropy, and derive corresponding variance estimators. We demonstrate their practical applicability by numerical experiments for the two-dimensional Ising model which highlight the superiority over existing methods. Our approach greatly enhances the applicability of generative neural samplers to real-world physical systems.}},
  pages     = {023304},
  number    = {2},
  volume    = {101},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2020/Physical%20Review%20E-2020-Nicoli.pdf}
}


@article{10.1103/physrevlett.128.090501,
  year      = {2022},
  title     = {{Autoregressive Neural Network for Simulating Open Quantum Systems via a Probabilistic Formulation}},
  author    = {Luo, Di and Chen, Zhuo and Carrasquilla, Juan and Clark, Bryan K.},
  journal   = {Physical Review Letters},
  issn      = {0031-9007},
  doi       = {10.1103/physrevlett.128.090501},
  abstract  = {{The theory of open quantum systems lays the foundation for a substantial part of modern research in quantum science and engineering. Rooted in the dimensionality of their extended Hilbert spaces, the high computational complexity of simulating open quantum systems calls for the development of strategies to approximate their dynamics. In this Letter, we present an approach for tackling open quantum system dynamics. Using an exact probabilistic formulation of quantum physics based on positive operator-valued measure, we compactly represent quantum states with autoregressive neural networks; such networks bring significant algorithmic flexibility due to efficient exact sampling and tractable density. We further introduce the concept of string states to partially restore the symmetry of the autoregressive neural network and improve the description of local correlations. Efficient algorithms have been developed to simulate the dynamics of the Liouvillian superoperator using a forward-backward trapezoid method and find the steady state via a variational formulation. Our approach is benchmarked on prototypical one-dimensional and two-dimensional systems, finding results which closely track the exact solution and achieve higher accuracy than alternative approaches based on using Markov chain Monte Carlo method to sample restricted Boltzmann machines. Our Letter provides general methods for understanding quantum dynamics in various contexts, as well as techniques for solving high-dimensional probabilistic differential equations in classical setups.}},
  pages     = {090501},
  number    = {9},
  volume    = {128},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2022/Physical%20Review%20Letters-2022-Luo_1.pdf}
}



@article{PhysRevA.102.062413,
  title     = {Calculating R\'enyi entropies with neural autoregressive quantum states},
  author    = {Wang, Zhaoyou and Davis, Emily J.},
  journal   = {Phys. Rev. A},
  volume    = {102},
  issue     = {6},
  pages     = {062413},
  numpages  = {11},
  year      = {2020},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevA.102.062413},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.102.062413}
}


@article{PixelCNN2016,
  year      = {2016},
  title     = {{Conditional Image Generation with PixelCNN Decoders}},
  author    = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  journal   = {arXiv},
  eprint    = {1606.05328},
  abstract  = {{This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.}},
  keywords  = {},
  local-url = {file://localhost/Users/ocadni/Documents/Papers%20Library/2016/arXiv-2016-Oord_2.pdf}
}
@inproceedings{NIPS2016_b1301141,
  author    = {van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and kavukcuoglu, koray and Vinyals, Oriol and Graves, Alex},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Conditional Image Generation with PixelCNN Decoders},
  url       = {https://proceedings.neurips.cc/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf},
  volume    = {29},
  year      = {2016}
}

@article{PhysRevLett.124.020503,
  title     = {Deep Autoregressive Models for the Efficient Variational Simulation of Many-Body Quantum Systems},
  author    = {Sharir, Or and Levine, Yoav and Wies, Noam and Carleo, Giuseppe and Shashua, Amnon},
  journal   = {Phys. Rev. Lett.},
  volume    = {124},
  issue     = {2},
  pages     = {020503},
  numpages  = {6},
  year      = {2020},
  month     = {Jan},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.124.020503},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.124.020503}
}

@article{condmat7020038,
  author         = {Inack, Estelle M. and Morawetz, Stewart and Melko, Roger G.},
  title          = {Neural Annealing and Visualization of Autoregressive Neural Networks in the Newman-Moore Model},
  journal        = {Condensed Matter},
  volume         = {7},
  year           = {2022},
  number         = {2},
  article-number = {38},
  url            = {https://www.mdpi.com/2410-3896/7/2/38},
  issn           = {2410-3896},
  abstract       = {Artificial neural networks have been widely adopted as ansatzes to study classical and quantum systems. However, for some notably hard systems, such as those exhibiting glassiness and frustration, they have mainly achieved unsatisfactory results, despite their representational power and entanglement content, thus suggesting a potential conservation of computational complexity in the learning process. We explore this possibility by implementing the neural annealing method with autoregressive neural networks on a model that exhibits glassy and fractal dynamics: the two-dimensional Newman&ndash;Moore model on a triangular lattice. We find that the annealing dynamics is globally unstable because of highly chaotic loss landscapes. Furthermore, even when the correct ground-state energy is found, the neural network generally cannot find degenerate ground-state configurations due to mode collapse. These findings indicate that the glassy dynamics exhibited by the Newman&ndash;Moore model caused by the presence of fracton excitations in the configurational space likely manifests itself through trainability issues and mode collapse in the optimization landscape.},
  doi            = {10.3390/condmat7020038}
}

@article{PhysRevE.103.012103,
  title     = {Solving statistical mechanics on sparse graphs with feedback-set variational autoregressive networks},
  author    = {Pan, Feng and Zhou, Pengfei and Zhou, Hai-Jun and Zhang, Pan},
  journal   = {Phys. Rev. E},
  volume    = {103},
  issue     = {1},
  pages     = {012103},
  numpages  = {12},
  year      = {2021},
  month     = {Jan},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.103.012103},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.103.012103}
}


@inproceedings{pmlr-v48-oord16,
  title     = {Pixel Recurrent Neural Networks},
  author    = {van den Oord, Aäron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages     = {1747--1756},
  year      = {2016},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v48/oord16.pdf},
  url       = {https://proceedings.mlr.press/v48/oord16.html},
  abstract  = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.}
}

@article{PhysRevResearch.2.023358,
  title     = {Recurrent neural network wave functions},
  author    = {Hibat-Allah, Mohamed and Ganahl, Martin and Hayward, Lauren E. and Melko, Roger G. and Carrasquilla, Juan},
  journal   = {Phys. Rev. Res.},
  volume    = {2},
  issue     = {2},
  pages     = {023358},
  numpages  = {17},
  year      = {2020},
  month     = {Jun},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevResearch.2.023358},
  url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.023358}
}


@inproceedings{pmlr-v15-larochelle11a,
  title     = {The Neural Autoregressive Distribution Estimator},
  author    = {Larochelle, Hugo and Murray, Iain},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages     = {29--37},
  year      = {2011},
  editor    = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume    = {15},
  series    = {Proceedings of Machine Learning Research},
  address   = {Fort Lauderdale, FL, USA},
  month     = {11--13 Apr},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf},
  url       = {https://proceedings.mlr.press/v15/larochelle11a.html},
  abstract  = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.  }
}

@article{PhysRevResearch.3.L042024,
  title     = {Unbiased Monte Carlo cluster updates with autoregressive neural networks},
  author    = {Wu, Dian and Rossi, Riccardo and Carleo, Giuseppe},
  journal   = {Phys. Rev. Res.},
  volume    = {3},
  issue     = {4},
  pages     = {L042024},
  numpages  = {7},
  year      = {2021},
  month     = {Nov},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevResearch.3.L042024},
  url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.L042024}
}

@article{PhysRevE.101.053312,
  title     = {Boosting Monte Carlo simulations of spin glasses using autoregressive neural networks},
  author    = {McNaughton, B. and Milo\ifmmode \check{s}\else \v{s}\fi{}evi\ifmmode \acute{c}\else \'{c}\fi{}, M. V. and Perali, A. and Pilati, S.},
  journal   = {Phys. Rev. E},
  volume    = {101},
  issue     = {5},
  pages     = {053312},
  numpages  = {12},
  year      = {2020},
  month     = {May},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.101.053312},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.101.053312}
}



@article{Carleo2018,
  abstract      = {Obtaining accurate properties of many-body interacting quantum matter is a long-standing challenge in theoretical physics and chemistry, rooting into the complexity of the many-body wave-function. Classical representations of many-body states constitute a key tool for both analytical and numerical approaches to interacting quantum problems. Here, we introduce a technique to construct classical representations of many-body quantum systems based on artificial neural networks. Our constructions are based on the deep Boltzmann machine architecture, in which two layers of hidden neurons mediate quantum correlations. The approach reproduces the exact imaginary-time evolution for many-body lattice Hamiltonians, is completely deterministic, and yields networks with a polynomially-scaling number of neurons. We provide examples where physical properties of spin Hamiltonians can be efficiently obtained. Also, we show how systematic improvements upon existing restricted Boltzmann machines ansatze can be obtained. Our method is an alternative to the standard path integral and opens new routes in representing quantum many-body states.},
  author        = {Carleo, Giuseppe and Nomura, Yusuke and Imada, Masatoshi},
  da            = {2018/12/14},
  date-added    = {2023-02-01 12:37:26 +0100},
  date-modified = {2023-02-01 12:37:26 +0100},
  doi           = {10.1038/s41467-018-07520-3},
  id            = {Carleo2018},
  isbn          = {2041-1723},
  journal       = {Nature Communications},
  number        = {1},
  pages         = {5322},
  title         = {Constructing exact representations of quantum many-body systems with deep neural networks},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/s41467-018-07520-3},
  volume        = {9},
  year          = {2018},
  bdsk-url-1    = {https://doi.org/10.1038/s41467-018-07520-3}
}

@article{Liu_2021,
  doi       = {10.1088/2632-2153/aba19d},
  url       = {https://dx.doi.org/10.1088/2632-2153/aba19d},
  year      = {2021},
  month     = {feb},
  publisher = {IOP Publishing},
  volume    = {2},
  number    = {2},
  pages     = {025011},
  author    = {Jin-Guo Liu and Liang Mao and Pan Zhang and Lei Wang},
  title     = {Solving quantum statistical mechanics with variational autoregressive networks and quantum circuits},
  journal   = {Machine Learning: Science and Technology},
  abstract  = {We extend the ability of an unitary quantum circuit by interfacing it with a classical autoregressive neural network. The combined model parametrizes a variational density matrix as a classical mixture of quantum pure states, where the autoregressive network generates bitstring samples as input states to the quantum circuit. We devise an efficient variational algorithm to jointly optimize the classical neural network and the quantum circuit to solve quantum statistical mechanics problems. One can obtain thermal observables such as the variational free energy, entropy, and specific heat. As a byproduct, the algorithm also gives access to low energy excitation states. We demonstrate applications of the approach to thermal properties and excitation spectra of the quantum Ising model with resources that are feasible on near-term quantum computers.}
}

@misc{https://doi.org/10.48550/arxiv.2210.11145,
  doi       = {10.48550/ARXIV.2210.11145},
  url       = {https://arxiv.org/abs/2210.11145},
  author    = {Ciarella, Simone and Trinquier, Jeanne and Weigt, Martin and Zamponi, Francesco},
  keywords  = {Disordered Systems and Neural Networks (cond-mat.dis-nn), FOS: Physical sciences, FOS: Physical sciences},
  title     = {Machine-learning-assisted Monte Carlo fails at sampling computationally hard problems},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{pmlr-v32-gregor14,
  title     = {Deep AutoRegressive Networks},
  author    = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  pages     = {1242--1250},
  year      = {2014},
  editor    = {Xing, Eric P. and Jebara, Tony},
  volume    = {32},
  number    = {2},
  series    = {Proceedings of Machine Learning Research},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v32/gregor14.pdf},
  url       = {https://proceedings.mlr.press/v32/gregor14.html},
  abstract  = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data.  Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.  We derive an efficient approximate parameter estimation method based on the minimum  description length (MDL) principle,  which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.   We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.}
}


@inproceedings{pmlr-v97-durkan19a,
  title     = {Autoregressive Energy Machines},
  author    = {Nash, Charlie and Durkan, Conor},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {1735--1744},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/durkan19a/durkan19a.pdf},
  url       = {https://proceedings.mlr.press/v97/durkan19a.html},
  abstract  = {Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.}
}


@inproceedings{pmlr-v80-huang18d,
  title     = {Neural Autoregressive Flows},
  author    = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {2078--2087},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/huang18d/huang18d.pdf},
  url       = {https://proceedings.mlr.press/v80/huang18d.html},
  abstract  = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.}
}


@article{Barrett2022,
  abstract      = {In recent years, neural-network quantum states have emerged as powerful tools for the study of quantum many-body systems. Electronic structure calculations are one such canonical many-body problem that have attracted sustained research efforts spanning multiple decades, whilst only recently being attempted with neural-network quantum states. However, the complex non-local interactions and high sample complexity are substantial challenges that call for bespoke solutions. Here, we parameterize the electronic wavefunction with an autoregressive neural network that permits highly efficient and scalable sampling, whilst also embedding physical priors reflecting the structure of molecular systems without sacrificing expressibility. This allows us to perform electronic structure calculations on molecules with up to 30 spin orbitals---at least an order of magnitude more Slater determinants than previous applications of conventional neural-network quantum states---and we find that our ansatz can outperform the de facto gold-standard coupled-cluster methods even in the presence of strong quantum correlations. With a highly expressive neural network for which sampling is no longer a computational bottleneck, we conclude that the barriers to further scaling are not associated with the wavefunction ansatz itself, but rather are inherent to any variational Monte Carlo approach.},
  author        = {Barrett, Thomas D. and Malyshev, Aleksei and Lvovsky, A. I.},
  da            = {2022/04/01},
  date-added    = {2023-02-01 14:29:34 +0100},
  date-modified = {2023-02-01 14:29:34 +0100},
  doi           = {10.1038/s42256-022-00461-z},
  id            = {Barrett2022},
  isbn          = {2522-5839},
  journal       = {Nature Machine Intelligence},
  number        = {4},
  pages         = {351--358},
  title         = {Autoregressive neural-network wavefunctions for ab initio quantum chemistry},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/s42256-022-00461-z},
  volume        = {4},
  year          = {2022},
  bdsk-url-1    = {https://doi.org/10.1038/s42256-022-00461-z}
}

@misc{https://doi.org/10.48550/arxiv.2005.14165,
  doi       = {10.48550/ARXIV.2005.14165},
  url       = {https://arxiv.org/abs/2005.14165},
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{kadanoff2000statistical,
  title     = {Statistical physics: statics, dynamics and renormalization},
  author    = {Kadanoff, Leo P},
  year      = {2000},
  publisher = {World Scientific}
}

@article{PhysRevLett.51.1206,
  title     = {Direct Determination of the Probability Distribution for the Spin-Glass Order Parameter},
  author    = {Young, A. P.},
  journal   = {Phys. Rev. Lett.},
  volume    = {51},
  issue     = {13},
  pages     = {1206--1209},
  numpages  = {0},
  year      = {1983},
  month     = {Sep},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.51.1206},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.51.1206}
}

@inproceedings{NEURIPS2019_bdbca288,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url       = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@misc{mygithub,
  author       = {Indaco Biazzo},
  title        = {{H2AR-NN}},
  howpublished = {\url{http://github.com/}},
  year         = {2023},
  note         = {[Online; accessed 1-march-2023]}
}

@article{doi:10.1063/1.1887186,
  author  = {Gubernatis,J. E. },
  title   = {Marshall Rosenbluth and the Metropolis algorithm},
  journal = {Physics of Plasmas},
  volume  = {12},
  number  = {5},
  pages   = {057303},
  year    = {2005},
  doi     = {10.1063/1.1887186},
  url     = { 
             https://doi.org/10.1063/1.1887186
             
             },
  eprint  = { 
             https://doi.org/10.1063/1.1887186
             
             }
}

@article{Cha_2022,
  doi       = {10.1088/2632-2153/ac362b},
  url       = {https://dx.doi.org/10.1088/2632-2153/ac362b},
  year      = {2021},
  month     = {nov},
  publisher = {IOP Publishing},
  volume    = {3},
  number    = {1},
  pages     = {01LT01},
  author    = {Peter Cha and Paul Ginsparg and Felix Wu and Juan Carrasquilla and Peter L McMahon and Eun-Ah Kim},
  title     = {Attention-based quantum tomography},
  journal   = {Machine Learning: Science and Technology},
  abstract  = {With rapid progress across platforms for quantum systems, the problem of many-body quantum state reconstruction for noisy quantum states becomes an important challenge. There has been a growing interest in approaching the problem of quantum state reconstruction using generative neural network models. Here we propose the ‘attention-based quantum tomography’ (AQT), a quantum state reconstruction using an attention mechanism-based generative network that learns the mixed state density matrix of a noisy quantum state. AQT is based on the model proposed in ‘Attention is all you need’ by Vaswani et al (2017 NIPS) that is designed to learn long-range correlations in natural language sentences and thereby outperform previous natural language processing (NLP) models. We demonstrate not only that AQT outperforms earlier neural-network-based quantum state reconstruction on identical tasks but that AQT can accurately reconstruct the density matrix associated with a noisy quantum state experimentally realized in an IBMQ quantum computer. We speculate the success of the AQT stems from its ability to model quantum entanglement across the entire quantum system much as the attention model for NLP captures the correlations among words in a sentence.}
}

@article{10.2307/20159953,
  issn      = {0003486X},
  url       = {http://www.jstor.org/stable/20159953},
  abstract  = {Using Guerra's interpolation scheme, we compute the free energy of the Sherrington-Kirkpatrick model for spin glasses at any temperature, confirming a celebrated prediction of G. Parisi.},
  author    = {Michel Talagrand},
  journal   = {Annals of Mathematics},
  number    = {1},
  pages     = {221--263},
  publisher = {Annals of Mathematics},
  title     = {The Parisi Formula},
  urldate   = {2023-03-01},
  volume    = {163},
  year      = {2006}
}


