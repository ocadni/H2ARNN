%\documentclass[aps,physrev,10pt,floatfix,reprint]{revtex4-2}
\documentclass[preprint, 10pt]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage[%Uncomment any one of the following lines to test 
scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage{esint}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{nicefrac}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
%\geometry{verbose,lmargin=2cm,rmargin=2cm}

\begin{document}

%\title{Autoregressive neural network of spins system: the deepness of Mean-Field models}
\title{Autoregressive neural network architecture of the Boltzmann distribution: the cases of two body interacting systems and two mean-field models.}
\author{Biazzo, Indaco}
 \altaffiliation[Also at ]{Physics Department, XYZ University.}
 %Lines break automatically or can be forced with \\
\date{\today}

\begin{abstract}
    In recent years, generative neural networks have gained significant attention for both scientific and commercial purposes. Generative Autoregressive Neural Networks (AR-NN), like Transformers, have demonstrated exceptional performance in various image and language generation tasks. This study aims to provide a physical interpretation of AR-NN architectures by reformulating the Boltzmann distribution of a generic pairwise interacting Hamiltonian of binary variables in autoregressive form. The resulting deep AR-NN architecture has weights and biases of the first layer that correspond to the couplings and external fields of the Hamiltonian, respectively, and incorporates commonly used features in computer science literature such as residual connections and recurrent structures, but now with clear physical interpretation. However, a major limitation of these AR-NNs is that one of the hidden layers grows exponentially with system size, making them infeasible for realistic applications. To address this challenge, AR-NN architectures for two iconic mean-field statistical physics systems, namely the Curie-Weiss (CW) and the Sherrington-Kirkpatrick (SK) models, are derived. The exact AR-NN architecture of the CW model is derived for both finite and infinite numbers of variables, resulting in deep architectures with parameters that scale polynomially with system size. An approximate AR-NN architecture for the SK model is derived based on the k-step replica symmetric breaking (k-RSB) ansatz of the Parisi solution, transforming its exponentially large hidden layer into a deep structure, where each step of replica symmetric breaking corresponds to adding a linear layer and a non-linear operator. The number of parameters in this architecture scales polynomially with system size for finite k-RSB, paving the way to apply the Parisi solution of the SK model to single instances problem. The derived architectures are compared with other AR-NN architectures with an equivalent number of parameters for both models, and superior performance is observed over a wide range of physical phase space. This framework provides a means to derive new AR-NN architectures for different interacting systems and interpret existing ones from a physical perspective.
\end{abstract}
    
    %\keywords{Suggested keywords}%Use showkeys class option if keyword
                                  %display desired
    
\maketitle
\tableofcontents
\section{Introduction} 
The cross-fertilization between machine learning and statistical physics, in particular of disorder systems, has a long history \cite{doi:10.1073/pnas.79.8.2554, PhysRevA.32.1007}.
Recently, the development of deep neural network frameworks \cite{bengioNatureDeepLearning2015} have been applied to statistical physics problems \cite{RevModPhys.91.045002} spanning a wide range of domains, including quantum mechanics \cite{doi:10.1126/science.aag2302, Nieuwenburg2017}, 
classical statistical physics \cite{Carrasquilla2017, Wu2019}, chemical and biological physics \cite{noe2019boltzmann,jumper2021highly}, and complex inference problems \cite{Biazzo2022}.
On the other hand, techniques borrowed from statistical physics have been used to shed light on the behavior of Machine Learning algorithms \cite{doi:10.1080/00018732.2016.1211393, Nguyen2017}, and even to suggest training or architecture frameworks \cite{Chaudhari_2019, pmlr-v37-sohl-dickstein15}.
In last years, the introduction of deep generative autoregressive models \cite{pmlr-v37-germain15, NIPS2016_b1301141}, like transformers \cite{NIPS2017_3f5ee243}, has been a breakthrough in the field, generating images and text with a quality comparable to human-generated ones \cite{https://doi.org/10.48550/arxiv.2005.14165}. One of the main advantages of these models is that they can be trained in an unsupervised manner, and they can be used, thanks to ancestral sampling, to generate efficient new independent samples from a given probability distribution. 
The introduction of deep AR-NN was motivated as a flexible and general approach to generating according to a probability distribution learned from data \cite{pmlr-v32-gregor14, pmlr-v15-larochelle11a, pmlr-v48-oord16}. In classical statistical physics, the AR-NN was introduced, in a variational setting, as an improvement over the standard variational approach thanks to the high expressiveness of the AR-NN architecture \cite{Wu2019}, and quickly similar approaches are used in different contexts and domains of classical \cite{10.1103/physreve.101.023304,PhysRevE.101.053312,PhysRevE.103.012103,PhysRevResearch.3.L042024,10.1038/s42256-021-00401-3} and quantum statistical physics \cite{10.1103/physrevlett.128.090501,PhysRevA.102.062413,PhysRevLett.124.020503,PhysRevResearch.2.023358, Liu_2021, Barrett2022}. The ability of the AR-NNs to efficiently generate samples, thanks to the ancestral sampling procedure, opened the way to overcome the slow down of Monte-Carlo methods for frustrated or complex systems, although recently some work questioned the real gain in very frustrated systems \cite{condmat7020038,https://doi.org/10.48550/arxiv.2210.11145}.
%The main idea behind these models is to use a neural network to predict the next element in a sequence given all previous elements. 
A large part of the application of the AR-NN in statistical physics problems apart from small modifications used architecture created for computer science problems, usually constructed for very different purposes. The main difference when we are working with statistical physics problems is that usually the model we want to approximate is known, and the main aim of this work is to show how this information can be used to shape better architecture for the problems under study, and using standard statistical physics computations to derive them. In the following, I'll derive AR-NN architecture of the classical Boltzmann distribution of a general pairwise interacting Hamiltonians of binary variables. Despite the generality of the hamiltonian several interesting properties comes out:
\begin{itemize}
    \item The first layer parameters of the network are directly related to the Hamiltonian parameters.
    \item the appetence of residual connections and recurrent structures with a clear physical interpretation.
\end{itemize}
The resulting deep AR-NN architecture has the number of parameters of the second layer scaling exponentially with the system's size. 
However, the physical clear picture of the architecture allows us to use standard statistical physics techniques to find new AR-NN architecture for specific statistical physics systems. To show the potentiality of the representation the AR-NN architecture for two very well-known mean-field models is derived.

In particular, neural network architectures for two exact solvable models: the Curie-Weiss model (CW) and the Sherrington-Kirkpatrick model (SK) will be derived. These fully connected models are chosen due to their paradigmatic role in the history of statistical physics systems. The CW model, despite its straightforward hamiltonian, was one of the first model explaining the behavior of ferromagnet systems, displaying a second-order phase transition \cite{kadanoff2000statistical}. In this case, an exact AR-NN architecture both at finite N and in the thermodynamic limit is obtained both having the number of parameters scaling polinomially with the system's size.

The second is the Sherrington-Kirkpatrick (SK) model \cite{PhysRevLett.35.1792}, a spin glass Mean-Field model of disorder magnetic materials. Once again, despite its simple Hamiltonian, key concept and methods of the celebrated \cite{Nobel2021} replica symmetric breaking solution \cite{PARISI1979203, PhysRevLett.43.1754} of Parisi, has been applied to very different problems, like neural networks \cite{Gardner_1987, PhysRevLett.55.1530}, optimizations \cite{doi:10.1126/science.1073287}, inference problems \cite{doi:10.1080/00018732.2016.1211393}, or in characterizing the jamming of hard spheres \cite{RevModPhys.82.789, PhysRevLett.102.195701}. The complex many valley landscapes of the probability distribution captured by the replica symmetric breaking solution is the key concept that unifies the description of many different problems. In the following, I will derive an AR-NN architecture for the Boltzmann distribution of a single instance of disorder, with a finite number of variables, of the SK model. The derivation is based on the K-RSB solution, and intriguingly every step of the RSB solution corresponds to adding a linear layer and a non-linear operator, resulting in a deep AR-NN architecture with parameters scaling polinomially with the system size. 
Where?
\cite{Carleo2018},
similar approach for density estimation with energy function, where they arrive an approximated ARNN with pecularieties similar of my approach \cite{pmlr-v80-huang18d}.
            
\section{autoregressive Boltzmann distribution of pair-wise interacting Hamiltonian}

The Boltzmann probability distribution a generic Hamiltonian $H[\mathbf{x}]$ of a set of $N$ binary variables $\mathbf{x}=\{x_1, x_2,...x_N\}$ at inverse temperature $\beta$ is $P(\mathbf{x}) = \nicefrac{e^{-\beta H\left(\mathbf{x}\right)}}{Z}$, where $Z=\sum_{\mathbf{x}}e^{-\beta H\left(\mathbf{x}\right)}$ is the usually normalization factor.
It is generally challenging to compute marginals and average quantities when $N$ is large, and generate samples on frustrated systems. Defining the sets of variables $\mathbf{x}_{<i}=\left(x_{1},x_{2}\dots x_{i-1}\right)$ and $\mathbf{x}_{>i}=\left(x_{i+1},x_{i+2}\dots x_{N}\right)$ respectively with an index lower and larger than $i$, then if we can rewrite the Boltzmann distribution in the autoregressive form:
$
P\left(\mathbf{x}\right)=\prod_{i}P\left(x_{i}|\mathbf{x}_{<i}\right)
$
then it would be straightforward to produce independent samples from it, thanks to the ancestral sampling procedure. It has been proposed to use a variational approach to approximate the Boltzmann distribution with trial probability distributions that have this autoregressive form where each conditional probability is represented by a feed-forward neural network with a set of parameters ${\theta}$,
$
Q^{\theta}\left(\mathbf{x}\right)=\prod_{i}Q^{\theta_i}\left(x_{i}|\mathbf{x}_{<i}\right)
$.
The parameters ${\theta}$ can be learn minimizing the (inverse) Kullback-Leibler divergence $D_{KL}$,
with the true probability function:
\begin{equation}
\begin{split}
& D_{KL}\left(P|Q^{\Theta}\right) =  \sum_{\left\{ x\right\} }Q^{\Theta}\left(\mathbf{x} \right)\ln\left(\frac{Q^{\Theta}\left(\mathbf{x} \right)}{P\left(\mathbf{x} \right)}\right)  \\
& \approx \sum_{\mathbf{x}\sim Q^{\Theta}}\left[\ln\left(Q^{\Theta}\left(\mathbf{x} \right)\right)-\ln\left(P\left(\mathbf{x} \right)\right)\right]
\end{split}    
\end{equation}
Thanks to the ancestral sampling, we substituted the sum over all possible configurations with a subset of configurations extracted from the autoregressive trial functions, and usually, an annealing procedure is applied, starting from high temperature and slowly decreasing it.
In this framework, the choice of the architecture of the neural networks is crucial to obtain a good approximation of the Boltzmann distribution.

\subsection{Conditionals}
The generic $i$-th conditional probability factor of the Boltzmann can be rewrite in this form: 
\begin{equation}
    \label{eq:chain}
    \begin{split}
    & P\left(x_{i}|\mathbf{x}_{<i}\right)  = 
    \frac{P\left(\mathbf{x}_{<i+1}\right)}{P\left(\mathbf{x}_{<i}\right)}  = 
    \frac{\sum_{\mathbf{x}_{>i}}P\left(\mathbf{x}\right)}{\sum_{\mathbf{x}_{>i-1}}P\left(\mathbf{x}\right)} \\
    &=\frac{\sum_{\mathbf{x}_{>i}}e^{-\beta H}}{\sum_{\mathbf{x}_{>i-1}}e^{-\beta H}}  = 
    \frac{f\left(x_{i},\mathbf{x}_{<i}\right)}{\sum_{x_{i}}f\left(x_{i},\mathbf{x}_{<i}\right)}.
    \end{split}
\end{equation}
where I defined: $f\left(x_{i}=\pm 1,\mathbf{x}_{<i}\right) = \sum_{x_{i+1}\dots x_{N}}e^{-\beta H}\delta_{x_i, \pm1}$, and $\delta_{a,b}$ is the delta di kronecker function that is one when the two values $(a,b)$ coincides and zero otherwise. Usually, in the representation of the conditional probability $P\left(x_{i}=1|\mathbf{x}_{<i}\right)$ as a feed-forward neural network, the set of variable $\mathbf{x}_{<i}$ is the input, and the sigma function $\sigma(x)=\frac{1}{1+e^{-x}}$ is the last layer, assuring that the output is between $0$ and $1$. The probability $P\left(x_{i}=-1|\mathbf{x}_{<i}\right) = 1 - P\left(x_{i}=1|\mathbf{x}_{<i}\right)$ is straightforward to obtain. With simple algebraic manipulations, we can write: 
\begin{equation}
    \label{eq:sigma_log}
    \begin{split}
    & P\left(x_{i}=1|\mathbf{x}_{<i}\right) = \frac{f\left(1,\mathbf{x}_{<i}\right)}{\sum_{x_{i}}f\left(x_{i},\mathbf{x}_{<i}\right)}\\
    &= \sigma\left(\log\left[f\left(1,\mathbf{x}_{<i}\right)\right]-\log\left[f\left(-1,\mathbf{x}_{<i}\right)\right]\right)
    \end{split}
\end{equation}
Consider a generic two-body interaction Hamiltonian of binary spin variables $x_i \in \{-1,1\}$, $H = -\sum_{i<j} J_{ij} x_i x_j - \sum_{i} h_i x_i$, where $J_{ij}$ are the interaction couplings and $h_i$ are the external fields. Substituting it in eq.\ref{eq:sigma_log}, we obtain:
\begin{figure}[!ht]
    %\centering 
    \includegraphics[width=0.45\textwidth]{img/twoboghann.pdf}
    \caption{Neural network Architectures of conditional probability}
    \label{fig:arch}
\end{figure}
\begin{equation}
    \label{eq:conditional_ghann}
    \begin{split}
    & P\left(x_{i}=1|\mathbf{x}_{<i}\right) = \\
    & \sigma\left( 2 \beta \left(h_i + \sum_{s=1}^{i-1} J_{si} x_s\right) +\log(\rho_i^+) - \log(\rho_i^-)
    \right),   
    \end{split}
\end{equation}
where:
\begin{equation}
    \begin{split}
    \rho_i^{\pm}&[\mathbf{x}_{<i}]  = \sum_{\mathbf{x}_{>i}}  \exp \bigg(
    \beta\sum_{l=i+1}^{N} x_l \sum_{s=1}^{i-1} J_{sl} x_s +\\
    &\beta\sum_{l=i+1}^{N}\big( \pm J_{il}  + h_l \big) x_l 
    + \beta\sum_{l=i+1}^{N}\sum_{l'=l+1}^{N} J_{ll'} x_l x_{l'} \bigg)
\end{split}
\label{eq:rho_ghann}
\end{equation}
The elements in $H$ that depend only on $\mathbf{x}_{<i}$, $\sum_{s=1}^{i-1} h_s x_s + \sum_{s=1}^{i-1}\sum_{s'=s+1}^{i-1} J_{ss'} x_{s} x_{s'}$, cancel out.
The conditional probability, eq.\ref{eq:conditional_ghann}, can be interpreted as a feed-forward neural network, with the following architecture (see fig.\ref{fig:arch}) :
\begin{equation}
    %\begin{multline}
        Q^{\theta}\left(\mathbf{x}_{<i}\right) = 
     \sigma \bigg\{ x_i^1(\mathbf{x}_{<i})+\log\big[ \sum_{c} e^{b_c^+ + \sum_{l=i+1}^{N} w_{cl} x_{il}^1(\mathbf{x}_{<i})}\big] %\\
     +\log\big[ \sum_{c} e^{b_c^- + \sum_{l=i+1}^{N} w_{cl} x_{il}^1(\mathbf{x}_{<i})}\big] \bigg\},
    %\end{multline}
\end{equation}
where $x_i^1(\mathbf{x}_{<i})=\sum_{s=1}^{i-1} J_{si} x_s$ and $x_{il}^1=\sum_{s=1}^{i-1} J_{sl} x_s$ are the output of the first layer. 
Then, a second layer acts on the set of $x_{il}^1$ (see fig.\ref{fig:arch}). The $\sum_{c}$ is over the $2^{N-i}$ number of configurations of the set of $\mathbf{x}_{>i}$ variables. 
The parameters of the second layers are the $b_c^{\pm} = \beta\sum_{l=i+1}^N (\pm J_{il} + h_l + \sum_{l'=l+1}^N J_{ll'}x_{l'}) x_l $ and the $w_{cl}=x_l$, where $c$ is the index of the configuration of the set of $\mathbf{x}_{>i}$ variables. Then, the two functions $\rho^{\pm}$ are obtained by applying the non-linear operator $\log \Sigma (\mathbf{x}) = \log(\sum_i e^{x_i})$ at the output of the second layer (see fig.\ref{fig:arch}). 
As the last layer, the two $\rho^{\pm}$ and $x_i^1$ are summed, and the sigma function is applied. The whole architecture of the Boltzmann distribution of a 2-BOdy General Hamiltonian of an Autoregressive Neural Network (2BoGHANN) is depicted in fig.\ref{fig:arch}. The total number of parameters scales exponentially with the system size, making the sampling infeasible after a few spins.
 Nevertheless, the 2BoGHANN architecture shows some interesting futures:
\begin{itemize}
    \item The weights and biases of the first layers are the parameters of the Hamiltonian of the Boltzmann distribution.  As for the author known, it is the first time that a neural network architecture is proposed to represent the autoregressive Boltzmann probability distribution of a classical physical system where the weights and biases of the first layers are the parameters of the Hamiltonian. 
    \item Residual connections among layers, due to the $x_i^1$ variables, naturally emerge from the derivation. 
    The appearance of residual connections is recent \cite{10.48550/arxiv.1512.03385}, but became quickly a key element in the success of the ResNet and transformer architectures \cite{vaswani2017attention}, in classification and generative tasks. They were presented as a way to improve the training of deep neural networks avoiding the exploding and vanishing gradient problem. In our context, they represent the direct interactions among the variable $x_i$ and all the previous variables $\mathbf{x}_{<i}$. 
    On the other hand, the functions $\rho_i^{\pm}$ take into account the effect of the interactions among $\mathbf{x}_{<i}$ and $\mathbf{x}_{>i}$ on the variable $x_i$, and needs an exponential number of parameters in the feed-forward representation in the general case. 
    %This distinctions of clearly identification of the porpouse of the elements in the architecture of the neurla could help shed light on the nature of residual connections in neural network architectures. 
    \item Recurrent neural network had in the history of machine learning a central role in learning and generating task \cite{bengioNatureDeepLearning2015, https://doi.org/10.48550/arxiv.1506.00019}. Some AR-NN architecture used in statistical physics problems are recurrent\cite{10.1038/s42256-021-00401-3, PhysRevResearch.2.023358}. 
    It is easy to show that also the 2BoGHANN encodes a recursive structure. 
    Consider the first layer of the 2BoGHANN, see figure \ref{fig:arch}. The first layer is composed of the following set of linear operators on the input $x_{il}(\mathbf{x}_{<i})=\sum_{s=1}^{i-1} J_{si} x_s$ and $\omega_{il}=\sum_{s=1}^{i-1} J_{sl} x_s$ with $i<l<=N$. 
    The set of $\omega_{il}$ can be rewritten in recursive form observing that:
    \begin{equation}
    \omega_{il} = \omega_{i-1,l} + J_{i-1,l} x_{i-1}
    \end{equation}
    The neurons $\omega_{il}$ in the first layer of each conditional probability in the 2BoGHANN architecture depend on the $\omega_{i-1,l}$ of the previous conditional probability, lighting up the recurrent nature of the 2BoGHANN architecture.
\end{itemize}
%This dependence on the size of the system appears reasonable because, otherwise, it could be possible to sample to whatever pairwise hamiltonian in polynomial time, and as long we assume that $P\neq NP$ it could not be possible. 
%The computational cost of the sum over all the configuration of spins $x_l$ grows exponentially with the system's size making it unfeasible, after a few spins, the explicit computations. The idea is to find feed-forward neural network architectures representing these functions with a polynomial number of free parameters. \\
The number of parameters of the feed-forward neural network representations of the $\rho_i^{\pm}$ functions of the conditional probability of the variable $i$, eq.\ref{eq:rho_ghann}, scale exponentially with the systems size, proportionally to $2^{N-i}$. 
The $\rho_i^{\pm}$ function can be intepret as the partition function of a system, where the variables are $\mathbf{x}_{>i}$ and the fixed values of the variables $x_{<i}$ act like external fields.
In the following, I show how to use standard tools of statistical physics to derive AR-NN architecture.  \\

\begin{figure}[!h]
    \centering 
    \includegraphics[width=0.5\textwidth]{img/ann_img.pdf}
    \caption{Neural network Architectures of conditional probability}
    \label{fig:arch_old}
\end{figure}

\section{Models}
\subsection{Curie-Weiss model}

The Curie-Weiss model (CW) is a uniform, fully-connected Ising model. The Hamiltonian, with $N$ spins, is $H\left(\mathbf{x}\right)=-h\sum_{i=1}^{N}x_{i}-\frac{J}{N}\sum_{i<j}x_{i}x_{j}$. The conditional probability of a spin $i$, eq.\ref{eq:conditional_ghann}, of the CW model is:
\begin{multline}
P^{CW}\left(x_{i}=1|\mathbf{x}_{<i}\right) = 
\sigma\bigg( 
 2 \beta h + 2 \beta \frac{J}{N}\sum_{s=1}^{i-1}x_{s} + \\
 \log(\rho_i^+[\mathbf{x}_{<i}]) - \log(\rho_i^-[\mathbf{x}_{<i}])
\bigg),
\label{eq:conditional_cw}
\end{multline}
where:
\begin{equation}
\rho_i^{\pm}[\mathbf{x}_{<i}] \propto \sum_{\mathbf{x}_{>i}}e^{\beta \left(h\pm\frac{J}{N}+\frac{J}{N}\sum_{s=1}^{i-1}x_{s}\right)\sum_{l=i+1}^{N}x_{l}+\frac{\beta J}{2N}(\sum_{l=i+1}^{N}x_{l})^{2}} 
%\sum_{x_{i+1}\dots x_{N}} e^{\beta h_i^{\pm}[\mathbf{x}_{<i}]S_i +\frac{\beta J}{2N}S_{i}^{2}},
\label{eq:rho_cw_0}
\end{equation}
Defining $h_i^{\pm}[\mathbf{x}_{<i}] =h\pm\frac{J}{N}+\frac{J}{N}\sum_{s=1}^{i-1}x_{s}$, at given $\mathbf{x}_{<i}$, the eq. \ref{eq:rho_cw_0} is equivalent to the partition function of a CW model, with $N-i$ spins and external fields $h_i^{\pm}$. 
As shown in the Appendix, the summations over $\mathbf{x_{i>}}$ can be easily done, finding the following expression:

 \begin{eqnarray*}
 \rho_i^{\pm}[\mathbf{x}_{<i}] = \sum_{k=0}^{N-i} e^{a_{i,k}^{\pm} + b_{i,k}^{\pm} \sum_s x_s} 
\end{eqnarray*}
where we defined:
\begin{eqnarray}
\label{eq:params}
b_{i,k}^{\pm} & = & \log\left(\binom{N-i}{k}\right) + \frac{\beta J}{2N}\left(N-i-2k\right)^{2}+\left(N-i-2k\right)\left(\beta h \pm \frac{\beta J}{N}\right)\\
\omega_{i,k}^{\pm} & = & \frac{\beta J}{N}\left(N-i-2k\right).
\end{eqnarray}
The final feed-forward architecture of the Curie-Weiss Autoregressive Neural Network (CW-ARNN) architecture is:
\begin{multline}
\label{eq:curie_weiss_cond}
P^{CW}\left(x_{i}=+1|\mathbf{x}_{<i}\right)  =   \sigma \bigg[b_{0}+\omega_{0}\sum_{s=1}^{i-1}x_{s}\\
-\log\big(\sum_{k=0}^{N-i}e^{b_{i,k}^{+} + 
w_{i,k}^{+}\sum_{s=1}^{i-1}x_{s}}\big)+\log\big(\sum_{k=0}^{N-i}e^{b_{i,k}^{-} + w_{i,k}^{-}\sum_{s=1}^{i-1}x_{s}}\big)\bigg].
\end{multline}
where $b^0=2\beta h$, $\omega^0_i = \frac{2\beta J}{N}$ are the same, and so shared, among all the conditional probability functions, see fig.\ref{fig:curie_weiss}. Their parameters have an analytic dependence from the parameters $J$ and $h$ of the Hamiltonian of the systems. 
%The set of parameters ($b_i, b_i^{k\pm}, \omega_i, \omega_i^{k\pm}$) can be consider as free parameters trained to minimize the KL divergence with the true probability distribution. 
It is interesting to note the number of parameters of the CW-ARNN is $2+4(N-i)$; they decrease as $i$ increases. This result is, somehow, the opposite of what the neural network architecture usually should take care of, meaning increasing the number of input variables, the number of parameters should also increase to describe the complexity of the function. Still, it is compatible with what was derived for the general case, where the number of parameters needed for the $\rho^{\pm}$ decreases, in that case exponentially, with the index $i$. The CW-ARNN depends only on the sum of the input variables.
The total number of parameters of all conditional probability distribution scales as $2N^2+ O(N)$. \\
If we consider the thermodynamical limit, $N \gg 1$, the architecture of the CW model $\text{CW-ARNN}_{\inf}$, simplify:
\begin{eqnarray}\
    \label{eq:curie_weiss_cond2}
    Q^{\Theta}\left(x_{i}=+1|\mathbf{x}_{<i}\right) & = & \sigma \left(b^0+\omega_{i}^0\sum_{s=1}^{i-1}x_{s} + \omega_i^1 \text{sign}(\sum_{s=1}^{i-1}x_{s})\right).
    \end{eqnarray}
where $b^0=2\beta h$, $\omega^0_i = \frac{2\beta J}{N}$ are the same, and so shared, among all the conditional probability functions. The $\omega^1_i = -2\beta J |m_i|$ is different for each of them, and it is the solution of the following equation:
\begin{equation}
    \frac{m_i N}{N-1} = \tanh \left( \beta(\frac{m_i N}{N - i} - \frac{m_{\beta}N}{N-i}) \right)
    \label{eq:extrem_i}
    \end{equation}
where $m_{\beta}$ is the average magnetization of the CW systems at inverse temperature $\beta$, see Appendix for details. In practice the set of $\omega^1_i$ are treated as variational parameters to be learned during the training. The total number of parameters of the $\text{CuWANN}_{\infty}$ scale as $N+2$.
    

\subsection{The SK model}
The SK hamiltonian, with zero external fields for simplicity, is given by:
\begin{equation}
H\left(\mathbf{x}\right)=-\sum_{i<j}J_{ij}x_{i}x_{j}
\end{equation}
where the set of $\underline{J}$ are i.i.d. random variable extracted from a Gaussian probability distribution $P(J)=\sqrt{\frac{N}{2\pi}}\exp\left(\frac{-NJ^2}{2} \right)$. \\
To find a feed-forward representation of conditional probability of its Boltzmann distribution, we use the replica trick \cite{10.1142/0271}, usually, used together with the average over the system's disorder. In our case, we work with a single instance of the set of $Js$, but we assume that $N-i$ is large enough such that the following approximation hold for $\rho_i^{\pm}$ functions: 
\[
\log\rho_i^{\pm} \sim \mathbb{E}\left[  \log\rho_i^{\pm} \right] = \lim_{n\rightarrow 0} \frac{  \log(\mathbb{E}\left[(\rho_i^{\pm})^n \right])}{n}
\]
In the last equality, we use the replica trick. 
For each specific conditional probability, the average over the disorder $\mathbb{E}$ is taken on the coupling variables $J_{ll'}$ with $l,l'>i$. Implicitly, we assume that the quantities $\log\rho_i^{\pm}$ are a self-averaged quantity on the $\mathbf{x}_{i>}$ variables.
 The replica computation can be carried on easily, finding:
\begin{multline}
\mathbb{E}_{\underline{J}}\left[(\rho_i^{\pm}[\mathbf{x}_{<i}])^n \right]  = \\
\int \prod_{l<l'} dP_{J_{ll'}} \bigg\{ 
\sum_{\{\underline{x}^{a}\}_{i+1}^N} \exp\bigg[\beta \bigg(
\sum_{l,a}\bigg( \pm J_{il} + \sum_{s} J_{sl} x_s \bigg) x_l^{a} + 
\sum_{l,l', a} J_{ll'} x_l^{a} x_{l'}^{a}
\bigg)  \bigg] 
\bigg\} \propto
\end{multline}
where, the sums over $(l,l')$, $s$ and $a$ run respectively over $(i+1,N)$, $(1,i-1)$ and $(1,n)$, and $dP_{J_{ll'}}=P(J_{ll'})dJ_{ll'}$. Defining $h_l^{\pm}[\mathbf{x}_{<i}] =\pm J_{il} + \sum_{s=1}^{i-1} J_{sl} x_s$ as an external field we can observe that the above quantity is the partition function a classical SK model with external fields $h_l^{\pm}[\mathbf{x}_{<i}]$, at fixed $\mathbf{x}_{<i}$ and $J_{ll'}$ as coupling constants of the variables $\mathbf{x}_{i>}$.  
Computing the integrals over the disorder we find: 
\begin{widetext}
\begin{multline}
\mathbb{E}_{\underline{J}}\left[(\rho_i^{\pm}[\mathbf{x}_{<i}])^n \right] =  \\
 \propto  
\sum_{\{\underline{x}^{a}\}_{i+1}^N} 
\exp\left\{\beta \left[
\sum_{l} h_l^{\pm}[\mathbf{x}_{<i}] \sum_{a} x_l^{a} +\frac{\beta}{2N} \sum_{a<b} \left( \sum_{l}  x_l^{a} x_l^{b} \right)^2 \right]  \right\}\\
 \propto  \int \prod_{a<b} dQ_{ab} e^{-\frac{N}{2}\beta^2Q_{a,b}^2}
\prod_{l} \left[
\sum_{\{\underline{x}^{a}_l\}} 
\exp\left\{\beta \left[
h_l^{\pm}[\mathbf{x}_{<i}] \sum_{a} x_l^{a} +\beta \sum_{a<b} Q_{a,b}  x_l^{a} x_l^{b} \right]  \right\}
\right]
\end{multline}
\end{widetext}
where in the last line we used the Hubbard-Stratonovich transformation to linearize the quadratic terms. 
The Parisi solutions of the SK model prescribe how to parametrize the matrix of the overlaps $\{Q_{a,b}\}$ \cite{10.1142/0271}. The easiest way to parametrize the overlaps is the replica symmetric solutions (RS), where the overlaps are symmetric under permutations: 
$$
Q_{a,b}=\begin{cases}
			0, & \text{if $a=b$}\\
            q, & \text{otherwise}
		 \end{cases},
$$
Then a sequence of better approximations can be obtained by breaking, by step, the replica symmetry, from the 1-step replica symmetric broken (1-RSB) to the infinite limit of the k-step replica symmetric broken (k-RSB) solutions. The parametrization of the overlap matrix allow to perform the sum over all the configuration of the variables $\mathbf{x}_{i>}$ getting rid of the exponential scales with the systems size of the number of parameters. The final AR-NN architecture of the SK model $\text{SK-ARNN}$ is: 
\begin{multline}
    Q^{\text{RS/k-RSB}}\left(x_{i}=1|\mathbf{x}_{<i}\right) = \sigma\left( 
        x_i^1(\mathbf{x}_{<i}) +\log(\rho_i^{+, \text{(RS/k-RSB)}}) - \log(\rho_i^{-, \text{(RS/k-RSB)}})
    \right) \\
\end{multline}.
where the RS case is $\log\sigma(b_1^{i,l^-} + w_1^{i,l^-} x_{i,l^-}^1(\mathbf{x}_{<i}$
of the RS solution of the $\log \rho^{RS,\pm}$ is:
The first one is the replica symmetric solution, then a hierarchical approach is used, where the replica symmetry between replica is broken, and the exact solution is obtained in the infinite limit of the replica symmetric  The following shows how to obtain neural network architectures based on the replica symmetric (RS) and k-step replica symmetric broken (k-RSB) solutions.


\subsubsection{Replica Symmetric solution (RS)}
We assume that the overlaps between the replicas are symmetric under permutations, and the matrix of the overlaps between replicas is parametrized with only one variable $q$:
$$
Q_{a,b}=\begin{cases}
			0, & \text{if $a=b$}\\
            q, & \text{otherwise}
		 \end{cases},
$$
obtaining:
\begin{widetext}
\begin{eqnarray}
\mathbb{E}_{\underline{J}}\left[(\rho_i^{\pm, sym}[\mathbf{x}_{<i}])^n \right] & = & 
c(n,N,i)
\int dq e^{-\frac{n(n-1)N}{4}\beta^2 q^2}
\prod_{l} \left[
\sum_{\{\underline{x}^{a}_l\}} 
\exp\left\{\beta \left[
h_l^{\pm}[\mathbf{x}_{<i}] \sum_{a} x_l^{a} +\beta q \sum_{a<b} x_l^{a} x_l^{b} \right]  \right\} 
\right] \\
& = &
c(n,N,i)
\int dq e^{-\frac{n(n-1)N}{4}\beta^2 q^2}
e^{-\frac{nN\beta^2 q}{2}}
\prod_{l} \left[
\sum_{\{\underline{x}^{a}_l\}} 
e^{\beta \left[
h_l^{\pm}[\mathbf{x}_{<i}] \sum_{a} x_l^{a} + \frac{\beta q}{2} \left(\sum_{a} x_l^{a} \right)^2 \right]} 
\right]\\
& = &
c'(n,N,i)
\int dq e^{-\frac{n(n-1)N}{4}\beta^2 q^2}
e^{-\frac{nN\beta^2 q}{2}}
\prod_{l} \left[\int \frac{dz_l}{\sqrt{2\pi q}} e^{-\frac{z_l^2}{q}}
\sum_{\{\underline{x}^{a}_l\}} 
e^{\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right) \sum_{a} x_l^{a}} 
\right]\\
& = &
c'(n,N,i)
\int dq e^{-nN\left(\frac{(n-1)}{4}\beta^2 q^2 +\frac{\beta^2 q}{2}\right)}
\prod_{l} \left[\int \frac{dz_l}{\sqrt{2\pi q}} e^{-\frac{z_l^2}{q}}
2^n\cosh^n \left(\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right)\right) 
\right].\\
\end{eqnarray}
\end{widetext}
Using the limit that $n\rightarrow 0$ we can write:
\begin{widetext}
\begin{eqnarray}
\int \frac{dz_l}{\sqrt{2\pi q}} e^{-\frac{z_l^2}{q}}
2^n\cosh^n \left(\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right)\right) = e^{n \int \frac{dz_l}{\sqrt{2\pi q}} e^{-\frac{z_l^2}{q}}
\log 2\cosh \left(\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right)\right)}.
\label{eq:gauss_n0}
\end{eqnarray}
\end{widetext}
obtaining:
\begin{widetext}
\begin{eqnarray}
\log (\rho_i^{\pm, sym}[\mathbf{x}_{<i}]) & = & 
\lim_{n\rightarrow 0} \frac{1}{n} \log \left( c'(n,N,i)
\int dq e^{-\frac{n(n-1)N}{4}\beta^2 q^2}
e^{-\frac{nN\beta^2 q}{2}}
e^{n \sum_l 
\int \frac{dz_l}{\sqrt{2\pi q}} e^{-\frac{z_l^2}{q}}
\log 2\cosh \left(\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right)\right)
} 
\right)\\
& = &
\log(c''(N,i)) + 
\left( +\frac{N}{4}\beta^2 q^2_0 
-\frac{N\beta^2 q_0}{2}
+ \sum_l 
\int \frac{dz_l}{\sqrt{2\pi q_0}} e^{-\frac{z_l^2}{q_0}}
\log 2\cosh \left(\beta \left(
h_l^{\pm}[\mathbf{x}_{<i}] +\beta z_l \right)\right)
\right) \\
& \doteq &  
c(N,i, q_0) -
\sum_l 
\int \frac{dz_l}{\sqrt{2\pi q_0}} e^{-\frac{z_l^2}{q_0}}
\log \sigma \left(\beta \left(
2h_l^{\pm}[\mathbf{x}_{<i}] +2\beta z_l \right)\right)
\end{eqnarray}
\end{widetext}
In the second line we use the saddle point methods to evaluate the integral over $q$, assuming that the single maximum value $q_0$ does not depend on the input values $\mathbf{x}_{<i}$ in the set of $h_l^{\pm}[\mathbf{x}_{<i}]$. It is a bold assumption to be verified {\it a posteriori} on the goodness of the neural network architectures performances. 
In the third line, we use the identity $\log\cosh(x) = 2x - \log\sigma(2x)$ and the elements that are equals between $\log(\rho^+)$ and $\log(\rho^-)$ are simplified. We introduced the $\log\sigma$ non-linear operator for computational reason.

% We can, after some manipulations, obtain a more neural network friendly function:
% \begin{eqnarray}
% \log (\rho_i^{\pm, sym} [\underline{x_l}])^n] & \approx & 
% \text{Extr}_q \left( +\frac{N}{4}\beta^2 q^2 
% -\frac{N\beta^2 q}{2}
% + \sum_l 
% \int dz_l e^{-z_l^2}
% \log \cosh \left(\beta \left(
% h_l +\beta \sqrt{q}z_l \right)\right)
% \right) 
% \end{eqnarray}

Now we consider the following approximation of the Gaussian convolution:
\[
\int dz e^{-z^2}
\log \sigma \left(\beta \left(
h +\beta \sqrt{q}z \right)\right) \sim b_0 + w_0*\log \sigma(b_1 + w_1 h), 
\]
where $(b_0, w_0, b_1,w_1)$ are free parameters to be determined. In Supporting Material (SI) a numerical analysis of the correctness of this approximation is shown.  
Putting together all the pieces, we can parameterize the conditional probability as:
\begin{multline}
Q^{RS}\left(x_{i}=1|\mathbf{x}_{<i}\right) = \sigma\left( 
    x_i^1(\mathbf{x}_{<i}) +\log(\rho_i^+) - \log(\rho_i^-)
\right) \\
 = \sigma \bigg( x_i^1(\mathbf{x}_{<i}) + \sum_{l^+=i+1}^{N}  w_0^{i,l^+} \log\sigma(b_1^{i,l^+} +
 w_1^{i,l^+} x_{i,l^+}^1(\mathbf{x}_{<i}))+ \\
 + \sum_{l^-=i+1}^{N}  w_0^{i,l^-} \log\sigma(b_1^{i,l^-} + w_1^{i,l^-} x_{i,l^-}^1(\mathbf{x}_{<i})
 \bigg) 
\end{multline}
where the set of $(\mathbf{b},\mathbf{w})$ are free variational parameters to learn. 
%The $\sum_l$ considers all the elements together of the plus and minus $\rho^{\pm}$ function. 
\\DO the image of the nets.


\subsubsection{K-step Replica symmetric breaking (k-RSB)}
Assuming that the replica symmetry is broken, we can use the following ansatz called 1-step replica symmetric breaking (1RSB), where the overlaps between replicas are divided into $m$ blocks:
\begin{eqnarray}
    Q_{a,b}=\begin{cases}
			q_1, & \text{if } I(a/m)=I(b/m) \\
            q_0. & \text{if } I(a/m) \neq I(b/m).
		 \end{cases}
\end{eqnarray}
With the above ansatz, we can compute the following quantities:
\begin{align}
\begin{split}
    \sum_{ab} Q_{ab} x_{a} x_{b}  &  = \frac{1}{2} \bigg[ q_0 \left( \sum_{a}x_a\right)^2 +\\ 
& (q_1-q_0) \sum_{\text{blocks}}  \left( \sum_{a \in \text{block}}x_a\right)^2   - nq_1\bigg] 
\end{split}
\\
\sum_{ab} Q_{ab}^2 & =  n^2 q_0^2 + nm(q_1^2 - q_0^2) -n q_1^2.
\end{align}
The equation \ref{eq:before_ansaltz} becomes:
\begin{widetext}
\begin{align}
& \mathbb{E}_{\underline{J}} \left[(\rho_i^{\pm, 1RSB})^n \right] =  \\[1ex]
\begin{split}
& = c(n,N,i) \int dq_1 dq_0 e^{\frac{N}{2}\beta^2 \left[n^2 q_0^2 + nm(q_1^2 - q_0^2) -n q_1^2 \right]} 
\prod_{l} \bigg[ \sum_{\{\underline{x}^{a}_l\}} e^{ \beta \big[ h_l^{\pm} \sum_{a} x_l^{a} +\beta q_0 \left( \sum_{a} x_p^{a} \right)^2 + \beta (q_1-q_0) \sum_{\text{blocks}} \left( \sum_{a \in \text{block}}x_l^{a}\right)^2  -n q_1 \bigl]}  \bigg] 
\end{split}\\ 
\begin{split}
& = c(n,N,i) \int dq_1 dq_0 e^{\frac{N}{2}\beta ^ 2 \left[n^2 q_0^2 + nm(q_1^2 - q_0^2) -n q_1^2 -n q_1\right]} 
\prod_{l} \bigg[ \sum_{\{\underline{x}^{a}_l\}} \int dP_{z_l} \prod_{k=1}^{n/m} \int dP{y_{lk}}  e^{\beta \big[h_l^{\pm} \sum_{a} x_l^{a} + \beta z_l \sum_{a}x_l^{a} + \beta \sum_{\text{blocks}}  y_{lk} \sum_{a \in \text{block}}x_l^{a}\bigl]}  \bigg] 
\end{split}\\ 
\begin{split}
& = c(n,N,i) \int dq_1 dq_0 e^{\frac{N}{2}\beta^2 \left[n^2 q_0^2 + nm(q_1^2 - q_0^2) -n q_1^2 -n q_1\right]} 
\prod_{l} \bigg[ \int dP_{z_l}  \prod_{k=1}^{n/m} \int dP_{y_{lk}} \cosh^m\bigg(\beta \big[h_l^{\pm}+ \beta z_l +\beta y_{lk}\bigl]  \bigg)  \bigg]
\end{split}\\ 
\begin{split}
& = c'(n,N,i) + c(n,N,i) \int dq_1 dq_0 e^{\frac{N}{2}\beta\left[n^2 q_0^2 + nm(q_1^2 - q_0^2) -n q_1^2 -n q_1\right]} 
\prod_{p} \int dP_{z_l}  \exp \bigg\{ \frac{n}{m} \log \bigg( \int dP_{y_{l}} \cosh^m\bigg(\beta \big[h_l^{\pm}+ \beta z_l + \beta y_{l}\bigl]  \bigg)  \bigg) \bigg\},
\end{split}\\ 
\end{align}
\end{widetext}
where we defined:
\begin{align}
    dP_{z_l} & = \frac{dz_l}{\sqrt{2\pi q_0}}e^{\frac{z^2}{2q_0}}\\
    dP_{y_{l}} & = \frac{dy_{l}}{\sqrt{2\pi (q_1-q_0)}}e^{\frac{y_{l}^2}{2 (q_1-q_0)}}.
\end{align}
Considering $N \gg 1$ and $n\rightarrow 0$ to use the saddle point methods and the identity in eq.\ref{eq:gauss_n0}, we can write:
\begin{widetext}
\begin{align}
\log (\rho_i^{\pm, 1RSB}) & = 
\lim_{n\rightarrow 0} \frac{1}{n} \log \left(\mathbb{E}_{\underline{J}} \left[(\rho_i^{\pm, 1RSB})^n \right]  \right) \\
& = c_i +  \text{Extr}_{q_0, q_1} \bigg[ c'_i(N,n,q_0, q_1) 
+ \frac{1}{m} \sum_{l} \int dP_{z_l} \log \bigg( \int dP_{y_{l}} \cosh^m\bigg(\beta \big[h_l^{\pm}+ \beta z_l + \beta y_{l}\big]  \bigg)  \bigg)
\bigg].
\end{align}
\end{widetext}

The above integrals are rewritten as the following:
\begin{align}
& \int dP_{z_l} \log \bigg( \int dP_{y_{l}}  \cosh^m\bigg(\beta \big[h_l^{\pm}+\beta z_l + \beta  y_{l}\big]  \bigg)  \bigg) 
 = \\
& \int dP_{z_l} \log \biggl( \int dP_{y_{l}} e^{ m \log \cosh \left(\beta \left[h_l^{\pm}+ \beta  z_l + \beta  y_{l}\right]  \right) } \biggr) 
 = \\
& \beta h_{l}^{\pm} + \int dP_{z_l} \log \biggl( \int dP_{y_{l}} e^{\beta^2 m y_{l} - m \log \sigma \left(\beta \left[h_l^{\pm}+ \beta z_l +\beta y_{l}\right]  \right) } \biggr) 
\end{align}
We have two nested gaussian convolutions. In order to make it easier to compute this non-linear operator, we will use a sequence of approximations similar to those used previously for RS case. %Recalling $h_l^{\pm}[\mathbf{x}_{<i}] =\pm J_{il} + \sum_{s} J_{sl} x_s$, we consider the approximation of the nested gaussian convolution:
%\begin{multline}
%f(\mathbf{x}_{<i}) = \int \frac{dz_l}{\sqrt{2\pi q_0}}e^{\frac{z^2}{2q_0}} \log \bigg( \int \frac{dy_{l}}{\sqrt{2\pi (q_1-q_0)}}e^{\frac{y_{l}^2}{2 (q_i-q_0)}} \\
% e^{ y_{l} - m \log \sigma \left(\beta \left[\pm J_{il} + \sum_{s} J_{sl} x_s + h + z_l + y_{l}\right]  \right) } \bigg) 
%\end{multline}
%Fixed the parameters of the model $(\{J_{pq}\}, h, \beta)$, this is a function that depends from three free parameters $(q_0, q_1, m)$. 
The integrals concerning the variables $(z_l, y_l)$ are approximated in the same way as the approach used previously for RS case: The number of free parameters increases to have feed-forward functions without integrals. First, we consider the following maps:
\begin{widetext}
\begin{align}
        & \int dP_{z_l}  \log \int dP_{y_{l}} e^{ \beta^2 m y_{l} - m \log \sigma \left(\beta \left[h_l^{\pm}+ z_l + y_{l}\right]  \right) }  \approx\\
        & \int dP_{z_l} \log \bigg( e^{\hat{b}_0^{l^{\pm}}}(1 + e^{\hat{b}_1^{l^{\pm}} + \hat{w}_1^{l^{\pm}} \log \sigma (\hat{b}_2^{l^{\pm}} + \hat{w}_2^{l^{\pm}} (h_l^{\pm}+ z_l)) }) \bigg) = \\
        & \hat{b}_0^{l^{\pm}} + \hat{w}_0^{l^{\pm}} \int dP_{z_l} \log \sigma \bigg(\hat{b}_1^{l^{\pm}} + \hat{w}_1^{l^{\pm}} \log \sigma (\hat{b}_2^{l^{\pm}} + \hat{w}_2^{l^{\pm}} (h_l^{\pm}+ z_l)) \bigg) \approx \\
        & b_0^{l^{\pm}} + w_0^{l^{\pm}} \log \sigma (b_1^{l^{\pm}} + w_1^{l^{\pm}} \log \sigma (b_2^{l^{\pm}} + w_2^{l^{\pm}} (h_l^{\pm}))),
%        \int dx e^{\frac{x^2}{a}} \log\sigma (a_1 + b_1 \log \sigma (a_2 + b_2 (h+x))) & \approx a_0 +b_0 \log \sigma (a'_1 + b'_1 \log \sigma (a'_2 + b'_2 (h))) 
\end{align}
\end{widetext}
where the set of parameters $(\mathbf{b_0^{{\pm}}},\mathbf{w_0^{{\pm}}},\mathbf{b_1^{{\pm}}},\mathbf{w_1^{{\pm}}},\mathbf{b_2^{{\pm}}},\mathbf{w_2^{{\pm}}})$ are free parameters to be determined by the learning procedures. The gaussian convolution integrals are substituted by feed-forward non-linear operations, enlarging the space of parameters. For the 1RSB case, we use the following architecture:
\begin{widetext}
\begin{multline}
    Q^{1RSB}\left(x_{i}=1|\mathbf{x}_{<i}\right) = \sigma\left( 
        x_i^1(\mathbf{x}_{<i}) +\log(\rho_i^+) - \log(\rho_i^-)
    \right) 
     = \sigma \bigg( x_i^1(\mathbf{x}_{<i}) + \\ \sum_{l^+=i+1}^{N}  w_0^{i,l^+} \log\sigma(b_1^{i,l^+} + 
     w_1^{i,l^+} \log\sigma(b_2^{i,l^+} +
     w_2^{i,l^+}  x_{i,l^+}^1(\mathbf{x}_{<i})))+ \sum_{l^-=i+1}^{N}  w_0^{i,l^-} \log\sigma(b_1^{i,l^-} + w_1^{i,l^-} \log\sigma(b_2^{i,l^+} +
     w_2^{i,l^+} x_{i,l^-}^1(\mathbf{x}_{<i})))
     \bigg) 
\end{multline}   
\end{widetext}
The generalization of the $\rho$ parametrization to the K-RSB case is straightforward. For instance for 2-RSB we have:
\begin{multline}
    \log \rho^{\pm, 2RSB} \left(x_{i}=1|\mathbf{x}_{<i}\right)  =  
   \sum_{l^{\pm}} w_0^{i,l^{\pm}} \log\sigma(b_1^{i,l^{\pm}} +\\
   w_1^{i,l^{\pm}} \log\sigma(b_2^{i,l^{\pm}} +
    w_2^{i,l^{\pm}}( \log\sigma(b_3^{i,l^{\pm}} +
    w_3^{i,l^{\pm}}x_{i,l^{\pm}}^1(\mathbf{x}_{<i})))))
\end{multline}

\section{Results}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/CW_res.pdf}
    \caption{Results}
    \label{fig:curie_weiss}
\end{figure}

\begin{figure}[]
    \centering 
    \includegraphics[width=0.5\textwidth]{img/SK_res.pdf}
    \caption{Results}
    \label{fig:SK}
\end{figure}



%\include{Conclusions}

\section{Appendix}
%\include{appendix}

\bibliography{refs}% Produces the bibliography via BibTeX.

\end{document}
  